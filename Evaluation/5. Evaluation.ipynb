{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d22c02a2-3a99-44bd-a690-901316f891ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit, cross_val_predict, cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, f1_score, precision_score, recall_score, roc_curve, auc, \n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "from pyswarm import pso\n",
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56f02e-3d29-4c92-822a-ceb039da49fe",
   "metadata": {},
   "source": [
    "## Evaluation <a id=\"evaluation\"></a>\n",
    "[go back to the top](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ea8fa-ffed-456f-bc06-2908fdaa1924",
   "metadata": {},
   "source": [
    "**Comparative Analysis of Machine Learning Models Without Feature Selection**\n",
    "\n",
    "In this section, we conduct a statistical comparison of three machine learning models—**Random Forest (RF)**, **Logistic Regression (LR)**, and **Support Vector Machine (SVM)**—across multiple datasets. The goal is to evaluate and determine whether there are significant differences in performance among these models based on various evaluation metrics.\n",
    "\n",
    "We analyze the models using three distinct datasets:\n",
    "1. **Combined Dataset (Without Feature Selection)**\n",
    "2. **Radiomic Dataset (Without Feature Selection)**\n",
    "3. **Pylidc Dataset (Without Feature Selection)**\n",
    "\n",
    "**Performance Metrics**\n",
    "For each dataset, the models are assessed based on the following metrics:\n",
    "- **Accuracy:** Measures the proportion of correctly predicted instances out of all predictions made.\n",
    "- **F1 Score:** The harmonic mean of precision and recall, providing a balance between the two.\n",
    "- **Precision:** Indicates the proportion of true positive predictions among all positive predictions.\n",
    "- **Recall (Sensitivity):** Reflects the model's ability to identify actual positive cases.\n",
    "- **ROC-AUC (Receiver Operating Characteristic - Area Under Curve):** Assesses the model's ability to distinguish between classes across various threshold settings.\n",
    "\n",
    "**Statistical Comparison Method**\n",
    "To determine if the observed differences in performance metrics between the models are statistically significant, we employ the **Wilcoxon Signed-Rank Test**. This non-parametric test is suitable for comparing paired samples without assuming a normal distribution of the differences. The significance level is set at **0.05**, meaning that p-values below this threshold indicate a statistically significant difference between the models for the given metric.\n",
    "\n",
    "For each dataset, the following steps are performed:\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - **Loading Results:** Import the cross-validation results for each model from their respective CSV files.\n",
    "   - **Filtering Data:** Exclude summary rows such as 'mean', 'std', and 'test_set' to focus solely on per-fold performance metrics.\n",
    "   - **Alignment Check:** Ensure that the cross-validation folds are consistently aligned across all models to maintain the integrity of paired comparisons.\n",
    "\n",
    "2. **Pairwise Model Comparison:**\n",
    "   - **Metric Selection:** For each performance metric (Accuracy, F1 Score, Precision, Recall, ROC-AUC), perform comparisons.\n",
    "   - **Statistical Testing:** Execute the Wilcoxon Signed-Rank Test for each pair of models:\n",
    "     - **RF vs. LR**\n",
    "     - **RF vs. SVM**\n",
    "     - **LR vs. SVM**\n",
    "   - **Result Interpretation:** Report the Wilcoxon test statistic and p-value for each comparison to assess statistical significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70373319-c66a-4f39-ade9-82a09cf5b586",
   "metadata": {},
   "source": [
    "**Combined Dataset Without Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8223ed47-875b-4348-a6ab-510332d8cdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing models for metric: Accuracy\n",
      "--------------------------------------------------\n",
      "Random Forest vs. Logistic Regression:\n",
      "  Wilcoxon Statistic = 13.0, p-value = 0.16016\n",
      "Random Forest vs. SVM:\n",
      "  Wilcoxon Statistic = 14.5, p-value = 0.19336\n",
      "Logistic Regression vs. SVM:\n",
      "  Wilcoxon Statistic = 13.5, p-value = 0.52709\n",
      "\n",
      "Comparing models for metric: F1\n",
      "--------------------------------------------------\n",
      "Random Forest vs. Logistic Regression:\n",
      "  Wilcoxon Statistic = 10.0, p-value = 0.08398\n",
      "Random Forest vs. SVM:\n",
      "  Wilcoxon Statistic = 11.0, p-value = 0.10547\n",
      "Logistic Regression vs. SVM:\n",
      "  Wilcoxon Statistic = 15.0, p-value = 0.23242\n",
      "\n",
      "Comparing models for metric: Precision\n",
      "--------------------------------------------------\n",
      "Random Forest vs. Logistic Regression:\n",
      "  Wilcoxon Statistic = 25.0, p-value = 0.84570\n",
      "Random Forest vs. SVM:\n",
      "  Wilcoxon Statistic = 22.0, p-value = 0.62500\n",
      "Logistic Regression vs. SVM:\n",
      "  Wilcoxon Statistic = 9.0, p-value = 0.06445\n",
      "\n",
      "Comparing models for metric: Recall\n",
      "--------------------------------------------------\n",
      "Random Forest vs. Logistic Regression:\n",
      "  Wilcoxon Statistic = 5.0, p-value = 0.01953\n",
      "Random Forest vs. SVM:\n",
      "  Wilcoxon Statistic = 1.0, p-value = 0.01729\n",
      "Logistic Regression vs. SVM:\n",
      "  Wilcoxon Statistic = 9.0, p-value = 0.06445\n",
      "\n",
      "Comparing models for metric: Roc_auc\n",
      "--------------------------------------------------\n",
      "Random Forest vs. Logistic Regression:\n",
      "  Wilcoxon Statistic = 7.0, p-value = 0.03711\n",
      "Random Forest vs. SVM:\n",
      "  Wilcoxon Statistic = 10.0, p-value = 0.08398\n",
      "Logistic Regression vs. SVM:\n",
      "  Wilcoxon Statistic = 27.0, p-value = 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitordrferreira/miniconda3/envs/lung_cancer_env/lib/python3.8/site-packages/scipy/stats/_morestats.py:3414: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/vitordrferreira/miniconda3/envs/lung_cancer_env/lib/python3.8/site-packages/scipy/stats/_morestats.py:3428: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "# Define the models and the metrics to compare\n",
    "models = {\n",
    "    'Random Forest': 'Combined Dataset (Without Feature Selection)_random_forest_results.csv',\n",
    "    'Logistic Regression': 'Combined Dataset (Without Feature Selection)_logistic_regression_results.csv',\n",
    "    'SVM': 'Combined Dataset (Without Feature Selection)_svm_results.csv'\n",
    "}\n",
    "\n",
    "# Define the metrics to compare\n",
    "metrics = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']\n",
    "\n",
    "# Load per-fold results for each model into a dictionary\n",
    "model_results = {}\n",
    "\n",
    "for model_name, filename in models.items():\n",
    "    df = pd.read_csv(filename)\n",
    "    # Exclude 'mean', 'std', and 'test_set' rows\n",
    "    exclude_rows = ['mean', 'std', 'test_set']\n",
    "    df = df[~df['fold'].isin(exclude_rows)].reset_index(drop=True)\n",
    "    model_results[model_name] = df\n",
    "\n",
    "# Ensure folds are aligned across models\n",
    "folds = model_results[next(iter(model_results))]['fold']\n",
    "for model_name, df in model_results.items():\n",
    "    assert all(df['fold'] == folds), f\"Folds are misaligned in {model_name}\"\n",
    "\n",
    "# Function to perform Wilcoxon Signed-Rank Test\n",
    "def compare_models(metric, model1_name, model2_name):\n",
    "    model1_scores = model_results[model1_name][metric]\n",
    "    model2_scores = model_results[model2_name][metric]\n",
    "    stat, p_value = wilcoxon(model1_scores, model2_scores)\n",
    "    return stat, p_value\n",
    "\n",
    "# Perform pairwise comparisons for each metric\n",
    "for metric in metrics:\n",
    "    print(f\"\\nComparing models for metric: {metric.capitalize()}\")\n",
    "    print('-' * 50)\n",
    "    for (model1_name, model2_name) in itertools.combinations(models.keys(), 2):\n",
    "        stat, p_value = compare_models(metric, model1_name, model2_name)\n",
    "        print(f\"{model1_name} vs. {model2_name}:\")\n",
    "        print(f\"  Wilcoxon Statistic = {stat}, p-value = {p_value:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969e20c-33ef-4f75-aa8a-6af58f2860e2",
   "metadata": {},
   "source": [
    "**Radiomic Dataset Without Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14497dee-ff92-450b-bb68-bf7f145c8e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing models for metric: Accuracy\n",
      "--------------------------------------------------\n",
      "Random Forest vs. Logistic Regression:\n",
      "  Wilcoxon Statistic = 17.5, p-value = 0.37500\n",
      "Random Forest vs. SVM:\n",
      "  Wilcoxon Statistic = 9.0, p-value = 0.06445\n",
      "Logistic Regression vs. SVM:\n",
      "  Wilcoxon Statistic = 15.0, p-value = 0.23242\n",
      "\n",
      "Comparing models for metric: F1\n",
      "--------------------------------------------------\n",
      "Random Forest vs. Logistic Regression:\n",
      "  Wilcoxon Statistic = 19.0, p-value = 0.43164\n",
      "Random Forest vs. SVM:\n",
      "  Wilcoxon Statistic = 11.0, p-value = 0.10547\n",
      "Logistic Regression vs. SVM:\n",
      "  Wilcoxon Statistic = 16.0, p-value = 0.27539\n",
      "\n",
      "Comparing models for metric: Precision\n",
      "--------------------------------------------------\n",
      "Random Forest vs. Logistic Regression:\n",
      "  Wilcoxon Statistic = 19.0, p-value = 0.43164\n",
      "Random Forest vs. SVM:\n",
      "  Wilcoxon Statistic = 10.0, p-value = 0.08398\n",
      "Logistic Regression vs. SVM:\n",
      "  Wilcoxon Statistic = 14.0, p-value = 0.19336\n",
      "\n",
      "Comparing models for metric: Recall\n",
      "--------------------------------------------------\n",
      "Random Forest vs. Logistic Regression:\n",
      "  Wilcoxon Statistic = 24.0, p-value = 0.76953\n",
      "Random Forest vs. SVM:\n",
      "  Wilcoxon Statistic = 19.0, p-value = 0.67840\n",
      "Logistic Regression vs. SVM:\n",
      "  Wilcoxon Statistic = 15.0, p-value = 0.37426\n",
      "\n",
      "Comparing models for metric: Roc_auc\n",
      "--------------------------------------------------\n",
      "Random Forest vs. Logistic Regression:\n",
      "  Wilcoxon Statistic = 17.0, p-value = 0.32227\n",
      "Random Forest vs. SVM:\n",
      "  Wilcoxon Statistic = 14.0, p-value = 0.19336\n",
      "Logistic Regression vs. SVM:\n",
      "  Wilcoxon Statistic = 0.0, p-value = 0.00195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitordrferreira/miniconda3/envs/lung_cancer_env/lib/python3.8/site-packages/scipy/stats/_morestats.py:3414: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/vitordrferreira/miniconda3/envs/lung_cancer_env/lib/python3.8/site-packages/scipy/stats/_morestats.py:3428: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "# Define the models and the metrics to compare\n",
    "models = {\n",
    "    'Random Forest': 'Radiomic Dataset (Without Feature Selection)_random_forest_results.csv',\n",
    "    'Logistic Regression': 'Radiomic Dataset (Without Feature Selection)_logistic_regression_results.csv',\n",
    "    'SVM': 'Radiomic Dataset (Without Feature Selection)_svm_results.csv'\n",
    "}\n",
    "\n",
    "# Define the metrics to compare\n",
    "metrics = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']\n",
    "\n",
    "# Load per-fold results for each model into a dictionary\n",
    "model_results = {}\n",
    "\n",
    "for model_name, filename in models.items():\n",
    "    df = pd.read_csv(filename)\n",
    "    # Exclude 'mean', 'std', and 'test_set' rows\n",
    "    exclude_rows = ['mean', 'std', 'test_set']\n",
    "    df = df[~df['fold'].isin(exclude_rows)].reset_index(drop=True)\n",
    "    model_results[model_name] = df\n",
    "\n",
    "# Ensure folds are aligned across models\n",
    "folds = model_results[next(iter(model_results))]['fold']\n",
    "for model_name, df in model_results.items():\n",
    "    assert all(df['fold'] == folds), f\"Folds are misaligned in {model_name}\"\n",
    "\n",
    "# Function to perform Wilcoxon Signed-Rank Test\n",
    "def compare_models(metric, model1_name, model2_name):\n",
    "    model1_scores = model_results[model1_name][metric]\n",
    "    model2_scores = model_results[model2_name][metric]\n",
    "    stat, p_value = wilcoxon(model1_scores, model2_scores)\n",
    "    return stat, p_value\n",
    "\n",
    "# Perform pairwise comparisons for each metric\n",
    "for metric in metrics:\n",
    "    print(f\"\\nComparing models for metric: {metric.capitalize()}\")\n",
    "    print('-' * 50)\n",
    "    for (model1_name, model2_name) in itertools.combinations(models.keys(), 2):\n",
    "        stat, p_value = compare_models(metric, model1_name, model2_name)\n",
    "        print(f\"{model1_name} vs. {model2_name}:\")\n",
    "        print(f\"  Wilcoxon Statistic = {stat}, p-value = {p_value:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c5c093-b058-4256-9b28-e90aca5ee241",
   "metadata": {},
   "source": [
    "**Pylidc Dataset Without Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5c905a8-d6f3-4832-81f1-1710266d9b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing models for metric: Accuracy\n",
      "--------------------------------------------------\n",
      "Random Forest vs. Logistic Regression:\n",
      "  Wilcoxon Statistic = 4.0, p-value = 0.01367\n",
      "Random Forest vs. SVM:\n",
      "  Wilcoxon Statistic = 5.0, p-value = 0.03815\n",
      "Logistic Regression vs. SVM:\n",
      "  Wilcoxon Statistic = 4.0, p-value = 0.09039\n",
      "\n",
      "Comparing models for metric: F1\n",
      "--------------------------------------------------\n",
      "Random Forest vs. Logistic Regression:\n",
      "  Wilcoxon Statistic = 5.0, p-value = 0.01953\n",
      "Random Forest vs. SVM:\n",
      "  Wilcoxon Statistic = 8.0, p-value = 0.04883\n",
      "Logistic Regression vs. SVM:\n",
      "  Wilcoxon Statistic = 2.0, p-value = 0.02506\n",
      "\n",
      "Comparing models for metric: Precision\n",
      "--------------------------------------------------\n",
      "Random Forest vs. Logistic Regression:\n",
      "  Wilcoxon Statistic = 14.0, p-value = 0.19336\n",
      "Random Forest vs. SVM:\n",
      "  Wilcoxon Statistic = 12.0, p-value = 0.13086\n",
      "Logistic Regression vs. SVM:\n",
      "  Wilcoxon Statistic = 15.0, p-value = 0.67442\n",
      "\n",
      "Comparing models for metric: Recall\n",
      "--------------------------------------------------\n",
      "Random Forest vs. Logistic Regression:\n",
      "  Wilcoxon Statistic = 2.0, p-value = 0.01516\n",
      "Random Forest vs. SVM:\n",
      "  Wilcoxon Statistic = 10.0, p-value = 0.08398\n",
      "Logistic Regression vs. SVM:\n",
      "  Wilcoxon Statistic = 0.0, p-value = 0.01172\n",
      "\n",
      "Comparing models for metric: Roc_auc\n",
      "--------------------------------------------------\n",
      "Random Forest vs. Logistic Regression:\n",
      "  Wilcoxon Statistic = 6.0, p-value = 0.02734\n",
      "Random Forest vs. SVM:\n",
      "  Wilcoxon Statistic = 6.0, p-value = 0.02734\n",
      "Logistic Regression vs. SVM:\n",
      "  Wilcoxon Statistic = 23.0, p-value = 0.69531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vitordrferreira/miniconda3/envs/lung_cancer_env/lib/python3.8/site-packages/scipy/stats/_morestats.py:3414: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/vitordrferreira/miniconda3/envs/lung_cancer_env/lib/python3.8/site-packages/scipy/stats/_morestats.py:3428: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    }
   ],
   "source": [
    "# Define the models and the metrics to compare\n",
    "models = {\n",
    "    'Random Forest': 'Pylidc Dataset (Without Feature Selection)_random_forest_results.csv',\n",
    "    'Logistic Regression': 'Pylidc Dataset (Without Feature Selection)_logistic_regression_results.csv',\n",
    "    'SVM': 'Pylidc Dataset (Without Feature Selection)_svm_results.csv'\n",
    "}\n",
    "\n",
    "# Define the metrics to compare\n",
    "metrics = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']\n",
    "\n",
    "# Load per-fold results for each model into a dictionary\n",
    "model_results = {}\n",
    "\n",
    "for model_name, filename in models.items():\n",
    "    df = pd.read_csv(filename)\n",
    "    # Exclude 'mean', 'std', and 'test_set' rows\n",
    "    exclude_rows = ['mean', 'std', 'test_set']\n",
    "    df = df[~df['fold'].isin(exclude_rows)].reset_index(drop=True)\n",
    "    model_results[model_name] = df\n",
    "\n",
    "# Ensure folds are aligned across models\n",
    "folds = model_results[next(iter(model_results))]['fold']\n",
    "for model_name, df in model_results.items():\n",
    "    assert all(df['fold'] == folds), f\"Folds are misaligned in {model_name}\"\n",
    "\n",
    "# Function to perform Wilcoxon Signed-Rank Test\n",
    "def compare_models(metric, model1_name, model2_name):\n",
    "    model1_scores = model_results[model1_name][metric]\n",
    "    model2_scores = model_results[model2_name][metric]\n",
    "    stat, p_value = wilcoxon(model1_scores, model2_scores)\n",
    "    return stat, p_value\n",
    "\n",
    "# Perform pairwise comparisons for each metric\n",
    "for metric in metrics:\n",
    "    print(f\"\\nComparing models for metric: {metric.capitalize()}\")\n",
    "    print('-' * 50)\n",
    "    for (model1_name, model2_name) in itertools.combinations(models.keys(), 2):\n",
    "        stat, p_value = compare_models(metric, model1_name, model2_name)\n",
    "        print(f\"{model1_name} vs. {model2_name}:\")\n",
    "        print(f\"  Wilcoxon Statistic = {stat}, p-value = {p_value:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff8b038-7474-4c19-a70d-fb0ce25641c4",
   "metadata": {},
   "source": [
    "### Define best model with weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9edd236-f8b0-4409-aa11-4f57c57b9d3e",
   "metadata": {},
   "source": [
    "To make an informed decision about the best model, we implemented a weighted scoring system based on several important performance metrics: Recall, ROC-AUC, F1-Score, Precision, and Accuracy. Each metric is assigned a specific weight according to its importance in the medical context of cancer diagnosis, where recall (avoiding false negatives) is of utmost importance, while ROC-AUC and F1-Score also play critical roles in model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "42caf1e5-cffe-4224-9620-ed29c7814300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 models based on Test weighted score:\n",
      "   model                                            dataset  weighted_score  \\\n",
      "12    RF         Pylidc Dataset (Without Feature Selection)        0.882663   \n",
      "15    RF          Combined Dataset (With Feature Selection)        0.879666   \n",
      "2    SVM  Combined Dataset (With Feature Selection by La...        0.852747   \n",
      "6    SVM       Combined Dataset (Without Feature Selection)        0.850782   \n",
      "16    RF       Combined Dataset (Without Feature Selection)        0.850563   \n",
      "\n",
      "    test_recall  test_roc_auc   test_f1  test_precision  test_accuracy  \n",
      "12     0.879479      0.955268  0.842434        0.808383       0.891631  \n",
      "15     0.859935      0.956081  0.846154        0.832808       0.896996  \n",
      "2      0.843648      0.940383  0.806854        0.773134       0.866953  \n",
      "6      0.840391      0.939429  0.804992        0.772455       0.865880  \n",
      "16     0.811075      0.945676  0.816393        0.821782       0.879828  \n",
      "\n",
      "Top 5 models based on CV weighted score:\n",
      "   model                                            dataset  \\\n",
      "12    RF         Pylidc Dataset (Without Feature Selection)   \n",
      "10    LR          Combined Dataset (With Feature Selection)   \n",
      "2    SVM  Combined Dataset (With Feature Selection by La...   \n",
      "6    SVM       Combined Dataset (Without Feature Selection)   \n",
      "3    SVM  Combined Dataset (With Feature Selection by Ra...   \n",
      "\n",
      "    cv_weighted_score  cv_recall_mean  cv_roc_auc_mean  cv_f1_mean  \\\n",
      "12           0.860771        0.828966         0.938073    0.834130   \n",
      "10           0.859434        0.829255         0.936944    0.832005   \n",
      "2            0.859346        0.832010         0.936024    0.830905   \n",
      "6            0.858442        0.834453         0.932898    0.829568   \n",
      "3            0.854163        0.817220         0.932354    0.829600   \n",
      "\n",
      "    cv_precision_mean  cv_accuracy_mean  \n",
      "12           0.840406          0.870837  \n",
      "10           0.835672          0.867814  \n",
      "2            0.830652          0.866909  \n",
      "6            0.825564          0.864199  \n",
      "3            0.843024          0.866915  \n"
     ]
    }
   ],
   "source": [
    "# Define the weights for each metric\n",
    "weights = {\n",
    "    'recall': 0.35,\n",
    "    'roc_auc': 0.25,\n",
    "    'f1': 0.25,\n",
    "    'precision': 0.1,\n",
    "    'accuracy': 0.05\n",
    "}\n",
    "# Calculate the weighted score for test set metrics\n",
    "df['weighted_score'] = (\n",
    "    (weights['recall'] * df['test_recall']) +\n",
    "    (weights['roc_auc'] * df['test_roc_auc']) +\n",
    "    (weights['f1'] * df['test_f1']) +\n",
    "    (weights['precision'] * df['test_precision']) +\n",
    "    (weights['accuracy'] * df['test_accuracy'])\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by the weighted score and get the top 5 models\n",
    "top_5_models_test = df.sort_values(by='weighted_score', ascending=False).head(5)\n",
    "\n",
    "print(\"Top 5 models based on Test weighted score:\")\n",
    "print(top_5_models_test[['model', 'dataset', 'weighted_score', 'test_recall', 'test_roc_auc', 'test_f1', 'test_precision', 'test_accuracy']])\n",
    "\n",
    "# Calculate the weighted score for cross-validation (CV) metrics\n",
    "df['cv_weighted_score'] = (\n",
    "    (weights['recall'] * df['cv_recall_mean']) +\n",
    "    (weights['roc_auc'] * df['cv_roc_auc_mean']) +\n",
    "    (weights['f1'] * df['cv_f1_mean']) +\n",
    "    (weights['precision'] * df['cv_precision_mean']) +\n",
    "    (weights['accuracy'] * df['cv_accuracy_mean'])\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by the CV weighted score and get the top 5 models\n",
    "top_5_models_cv = df.sort_values(by='cv_weighted_score', ascending=False).head(5)\n",
    "\n",
    "print(\"\\nTop 5 models based on CV weighted score:\")\n",
    "print(top_5_models_cv[['model', 'dataset', 'cv_weighted_score', 'cv_recall_mean', 'cv_roc_auc_mean', 'cv_f1_mean', 'cv_precision_mean', 'cv_accuracy_mean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e1c0e-8832-45a1-9d80-e0a78b393d50",
   "metadata": {},
   "source": [
    "**OUTRAS IDEIAS**:\n",
    "\n",
    "\n",
    "Analyze how the use of feature selection methods (Random Forest, Lasso, or no feature selection) affects model performance:\n",
    "Compare performance metrics with and without feature selection for each model.\n",
    "\n",
    "Examine how the model’s performance changes when trained on different datasets (Radiomic, Combined, Pylidc):\n",
    "\n",
    "Review the standard deviation (std) values to gauge the stability and consistency of each model's performance:\n",
    "High standard deviation indicates performance varies significantly across cross-validation folds, while low values imply stable performance.\n",
    "\n",
    "Analyze which models have the most consistent performance across metrics (i.e., low std in accuracy, F1, etc.).\n",
    "\n",
    "Compare the performance on the test set against cross-validation (CV) results:\n",
    "\n",
    "Look for overfitting (high CV accuracy but lower test accuracy) or underfitting (low CV and test accuracy).\n",
    "\n",
    "Boxplots for each metric across models and feature selection techniques.\n",
    "Line plots or bar plots to compare model performance on the test set versus CV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0f241-76a2-42da-b316-ebfbf397b712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
